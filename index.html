<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Detecting AI-Assisted Responses — Project Page</title>
  <link rel="icon" type="image/svg+xml" href="audio-wave.svg" />
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <style>
    html { scroll-behavior: smooth; }
    body { font-family: "IBM Plex Sans"}
    .anchor-offset { scroll-margin-top: 88px; }
  </style>
</head>
<body class="bg-gray-50 text-gray-900">
  <a href="#content" class="sr-only focus:not-sr-only focus:absolute focus:top-4 focus:left-4 z-50 bg-white px-3 py-2 rounded shadow">Skip to content</a>

  <!-- Top Nav / Header -->
  <header class="sticky top-0 z-40 bg-white/80 backdrop-blur border-b border-gray-200">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="h-16 flex items-center justify-between">
        <div class="flex items-center gap-3">
          <div class="h-8 w-8 rounded-lg bg-blue-600"><img src="audio-wave.svg" alt="Audio Wave" class="h-8 w-8"></div>
          <div>
            <p class="text-xs uppercase tracking-widest text-gray-500">Leiden University · LIACS</p>
            <h1 class="text-base font-semibold">Audio Processing & Indexing Project</h1>
          </div>
        </div>
        <div class="text-sm text-gray-600 hidden md:block" id="last-updated">
          Last updated: <span data-updated>—</span>
        </div>
      </div>
    </div>
  </header>

  <!-- Hero -->
  <section class="bg-gradient-to-b from-white to-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12">
      <div class="grid lg:grid-cols-3 gap-10 items-center">
        <div class="lg:col-span-2">
          <h2 class="text-3xl sm:text-4xl font-bold leading-tight">A Multimodal Pipeline for Detecting Authenticity in Spoken Responses</h2>
          <p class="mt-4 text-lg text-gray-700">Combines acoustic analysis, speech recognition, and AI text detection to identify whether interview responses are spontaneous or read from AI-generated scripts.</p>
          <div class="mt-6 flex flex-wrap gap-3">
            <a href="https://huggingface.co/spaces/ranamhamoud/Authenticity" target="_blank" class="inline-flex items-center rounded-lg border border-blue-600 px-4 py-2 text-blue-700 hover:bg-blue-50">Try Live Demo</a>
            <a href="https://github.com/ranamkhamoud/api-project-page" target="_blank" class="inline-flex items-center rounded-lg border border-gray-900 px-4 py-2 text-gray-900 hover:bg-gray-100">View on GitHub</a>
            <a href="#abstract" class="inline-flex items-center rounded-lg border border-gray-300 px-4 py-2 text-gray-700 hover:bg-gray-100">Details</a>
          </div>
        </div>
        <div class="lg:col-span-1">
          <div class="bg-white rounded-2xl shadow p-5 border border-gray-100">
            <h3 class="font-semibold">Project Members</h3>
            <ul class="mt-3 space-y-1 text-gray-700 text-sm">
              <li>Ahmad El Hage</li>
              <li>Alexander Scheerder</li>
              <li>Leon Monster</li>
              <li>Ranam Hamoud</li>
            </ul>
            <div class="mt-5 border-t pt-4">
              <h4 class="text-sm font-semibold text-gray-600">Quick Links</h4>
              <ul class="mt-2 text-sm space-y-2">
                <li><a class="text-blue-600 hover:underline" href="#abstract">Abstract</a></li>
                <li><a class="text-blue-600 hover:underline" href="#introduction">Introduction</a></li>
                <li><a class="text-blue-600 hover:underline" href="#design">System Design</a></li>
                <li><a class="text-blue-600 hover:underline" href="#data">Data</a></li>
                <li><a class="text-blue-600 hover:underline" href="#experimentation">Experiments</a></li>
                <li><a class="text-blue-600 hover:underline" href="#deliverables">Deliverables</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Main Layout -->
  <main id="content" class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-10">
    <div class="grid lg:grid-cols-12 gap-10">
      <!-- Content -->
      <section class="lg:col-span-9 space-y-12">
        <!-- Abstract -->
        <section id="abstract" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Abstract</h3>
          <p class="mt-2 text-gray-700">
            Large language models enable candidates to generate polished answers and read them aloud during spoken interviews. Existing detection methods address audio or text in isolation, leaving a gap for AI-generated but human-delivered responses.
          </p>
          <p class="mt-3 text-gray-700">
            We present a multimodal pipeline combining a CNN classifier for read/spontaneous speech, Whisper-based transcription with linguistic analysis, and AI-generated text detection. On 20 recordings from native and non-native speakers, individual components achieved 50–70% accuracy, while weighted fusion reached 90% accuracy.
          </p>
        </section>

        <!-- Introduction -->
        <section id="introduction" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Introduction</h3>
          <p class="mt-2 text-gray-700">
            Online assessments are widely used in recruitment, but LLMs can generate contextually appropriate answers in seconds. Candidates can prompt an LLM, get a polished answer, and read it aloud during interviews or in online settings.
          </p>
          <p class="mt-3 text-gray-700">
            Text-only tools cannot distinguish reading from spontaneous speech. Purely acoustic approaches don't address content authorship. This motivates a multimodal approach analyzing both <em>how</em> something is said and <em>what</em> is being said.
          </p>
        </section>

        <!-- System Overview -->
        <section id="design" class="anchor-offset">
          <h3 class="text-2xl font-semibold">System Overview & Architecture</h3>
          <p class="mt-2 text-gray-700">
            The pipeline has three stages: processing, analysis, and results. Input is audio (WAV, MP3, M4A, FLAC, OGG) between 30 seconds and 5 minutes.
          </p>
          
          <div class="mt-5 bg-white border border-gray-200 rounded-xl p-6">
            <!-- <h4 class="font-semibold text-lg mb-4">Pipeline Architecture Diagram</h4> -->
            <img src="assets/overview.jpg" alt="Multimodal authenticity pipeline architecture" class="w-full">
            <p class="mt-3 text-gray-600 text-sm">
              Audio is processed into mel-spectrograms, acoustic features, and Whisper transcripts. Weighted outputs (15% acoustic, 50% AI detection, 35% linguistic) combine into a composite authenticity score mapped to four verdict categories.
            </p>
          </div>

          <div class="mt-5 bg-white border border-gray-200 rounded-xl p-6">
            <h4 class="font-semibold text-lg mb-4">Pipeline Stages</h4>
            <div class="space-y-4">
              <div class="flex gap-4">
                <div class="flex-shrink-0 w-8 h-8 bg-blue-100 text-blue-700 rounded-full flex items-center justify-center font-semibold">1</div>
                <div>
                  <p class="font-medium">Processing Stage</p>
                  <p class="text-gray-600 text-sm">Extracts mel-spectrograms, acoustic features (tempo, pitch, energy, ZCR), and Whisper ASR transcripts with timestamps.</p>
                </div>
              </div>
              <div class="flex gap-4">
                <div class="flex-shrink-0 w-8 h-8 bg-blue-100 text-blue-700 rounded-full flex items-center justify-center font-semibold">2</div>
                <div>
                  <p class="font-medium">Analysis Stage</p>
                  <p class="text-gray-600 text-sm">ResNet18-based CNN classifies speech style. Prosody features evaluated via thresholds. Transcript analyzed for AI patterns and linguistic markers.</p>
                </div>
              </div>
              <div class="flex gap-4">
                <div class="flex-shrink-0 w-8 h-8 bg-blue-100 text-blue-700 rounded-full flex items-center justify-center font-semibold">3</div>
                <div>
                  <p class="font-medium">Results Stage</p>
                  <p class="text-gray-600 text-sm">Composite score categorized as: Authentic (≥0.70), Likely Authentic (0.50–0.70), Questionable (0.30–0.50), or Likely Inauthentic (&lt;0.30).</p>
                </div>
              </div>
            </div>
          </div>
        </section>

        <!-- Data Description -->
        <section id="data" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Data Description</h3>
          
          <div class="mt-5 bg-white border border-gray-200 rounded-xl p-5">
            <h4 class="font-semibold">IEMOCAP Corpus (Training Data)</h4>
            <p class="mt-2 text-gray-700 text-sm">
              The <a href="https://github.com/ranamkhamoud/api-project-page/blob/main/iemocap.zip" target="_blank" class="text-blue-600 hover:underline">IEMOCAP corpus</a> (~12 hours) contains scripted dialogues and improvised scenarios from ten actors, providing clear distinction between read and spontaneous speech for training the CNN classifier.
            </p>
          </div>

          <div class="mt-5 bg-white border border-gray-200 rounded-xl p-5">
            <h4 class="font-semibold">Internally Collected Speech Data (Test Data)</h4>
            <p class="mt-2 text-gray-700 text-sm">
              We collected <a href="https://github.com/ranamkhamoud/api-project-page/blob/main/samples_read_vs_spon.zip" target="_blank" class="text-blue-600 hover:underline">20 samples</a> from two speakers (L1 and L2 English): 10 spontaneous and 10 read responses. Read responses used ChatGPT-generated scripts. Spontaneous recordings were completed first to avoid exposure bias.
            </p>
          </div>
        </section>

        <!-- Experimentation -->
        <section id="experimentation" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Experiments and Evaluation</h3>
          

          
          <h4 class="mt-5 font-semibold">Full Pipeline Evaluation</h4>
          <p class="mt-2 text-gray-700">
            Read speech scored 0.19–0.28 (mean 0.22); spontaneous scored 0.23–0.75 (mean 0.46). A threshold of 0.285 yielded 90% accuracy (18/20 correct).
          </p>

          <div class="mt-5 bg-white border border-gray-200 rounded-xl p-5">
            <!-- <h4 class="font-semibold mb-3">Composite Authenticity Scores Distribution</h4> -->
            <img src="assets/composite_scores_plot.png" alt="Bar chart showing composite authenticity scores" class="w-full rounded-lg">
            <p class="mt-3 text-gray-600 text-sm">
              Composite scores for read (R1-R10) and spontaneous (S1-S10) samples. Dashed line shows decision threshold at 0.285.
            </p>
          </div>
          
          <div class="mt-5 grid sm:grid-cols-3 gap-4">
            <div class="bg-blue-50 border border-blue-200 rounded-xl p-4 text-center">
              <p class="text-3xl font-bold text-blue-700">90%</p>
              <p class="text-sm text-gray-600">Overall Accuracy</p>
            </div>
            <div class="bg-green-50 border border-green-200 rounded-xl p-4 text-center">
              <p class="text-3xl font-bold text-green-700">100%</p>
              <p class="text-sm text-gray-600">Read Speech Detection</p>
            </div>
            <div class="bg-amber-50 border border-amber-200 rounded-xl p-4 text-center">
              <p class="text-3xl font-bold text-amber-700">90%</p>
              <p class="text-sm text-gray-600">Spontaneous Detection</p>
            </div>
          </div>
          
          <h4 class="mt-5 font-semibold">Discussion</h4>
          <p class="mt-2 text-gray-700">
            No single component performs well enough alone. Text authenticity was the strongest discriminator; acoustic features add complementary prosodic cues. The L1 speaker achieved 100% accuracy; L2 speaker achieved 90% (one fluent spontaneous sample misclassified).
          </p>
        </section>

        <!-- Limitations & Ethics -->
        <section id="limitations" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Limitations & Ethical Considerations</h3>
          
          <div class="mt-5 bg-amber-50 border border-amber-200 rounded-xl p-5">
            <ul class="mt-3 space-y-2 text-gray-700 text-sm list-disc list-inside">
              <li>Only 20 samples from 2 speakers</li>
              <li>CNN trained on acted speech, not naturalistic interviews</li>
              <li>Highly articulate spontaneous speakers may be flagged</li>
              <li>English only; other languages untested</li>
            </ul>
          </div>

          <div class="mt-5 bg-red-50 border border-red-200 rounded-xl p-5">
            <h4 class="font-semibold text-red-900">Ethical Considerations</h4>
            <p class="mt-3 text-gray-700 text-sm">
              False positives can have serious consequences. The system may disproportionately affect formal speakers, non-native speakers, or neurodivergent individuals.
            </p>
            <p class="mt-3 text-gray-700 text-sm">
              <strong>This system should flag responses for human review, not make autonomous decisions.</strong> Voice data collection requires informed consent and secure handling.
            </p>
          </div>
        </section>

        <!-- Deliverables -->
        <section id="deliverables" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Deliverables</h3>
          <div class="mt-5 grid sm:grid-cols-2 lg:grid-cols-3 gap-5">
            <div class="bg-white border border-gray-200 rounded-2xl p-5 shadow-sm">
              <h4 class="font-semibold">Project Code</h4>
              <p class="mt-2 text-gray-700">Full Python implementation with CNN, Whisper, and AI detection.</p>
              <a href="https://github.com/ranamkhamoud/api-project-page" target="_blank" class="mt-4 inline-flex items-center justify-center rounded-lg bg-blue-600 px-4 py-2 text-white hover:bg-blue-700">View on GitHub</a>
            </div>
            <div class="bg-white border border-gray-200 rounded-2xl p-5 shadow-sm">
              <h4 class="font-semibold">Training Data</h4>
              <p class="mt-2 text-gray-700">IEMOCAP corpus for CNN training.</p>
              <a href="https://sail.usc.edu/iemocap/" target="_blank" class="mt-4 inline-flex items-center justify-center rounded-lg bg-blue-600 px-4 py-2 text-white hover:bg-blue-700">Request Data</a>
            </div>
            <div class="bg-white border border-gray-200 rounded-2xl p-5 shadow-sm">
              <h4 class="font-semibold">Test Dataset</h4>
              <p class="mt-2 text-gray-700">20 samples from native and non-native speakers.</p>
              <a href="https://github.com/ranamkhamoud/api-project-page/blob/main/samples_read_vs_spon.zip" target="_blank" class="mt-4 inline-flex items-center justify-center rounded-lg bg-blue-600 px-4 py-2 text-white hover:bg-blue-700">Download Samples</a>
            </div>
            <!-- <div class="bg-white border border-gray-200 rounded-2xl p-5 shadow-sm">
              <h4 class="font-semibold">Technical Paper</h4>
              <p class="mt-2 text-gray-700">Full methodology and evaluation documentation.</p>
              <a href="report.pdf" class="mt-4 inline-flex items-center justify-center rounded-lg bg-red-600 px-4 py-2 text-white hover:bg-red-700">Download PDF</a>
            </div> -->
          </div>
        </section>


        <!-- Related Links -->
        <section id="related-links" class="anchor-offset">
          <h3 class="text-2xl font-semibold">Related Links</h3>
            <div class="bg-white border border-gray-200 rounded-lg p-4">
              <h4 class="font-semibold text-sm">Tools & Frameworks</h4>
              <ul class="mt-2 space-y-1 text-sm text-blue-700">
                <li><a class="hover:underline" href="https://openai.com/research/whisper" target="_blank">OpenAI Whisper ASR</a></li>
                <li><a class="hover:underline" href="https://www.gradio.app/" target="_blank">Gradio Web Framework</a></li>
                <li><a class="hover:underline" href="https://pytorch.org/" target="_blank">PyTorch</a></li>
                <li><a class="hover:underline" href="https://librosa.org/" target="_blank">Librosa Audio Library</a></li>
              </ul>
            </div>
          </div>
        </section>
      </section>
    </div>
  </main>

  <!-- Footer -->
  <footer class="border-t border-gray-200 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8 text-sm text-gray-600 flex flex-col sm:flex-row items-start sm:items-center justify-between gap-4">
      <p>© <span id="year"></span> Leiden University · LIACS</p>
      <p>Audio Processing & Indexing Project</p>
    </div>
  </footer>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
    const updated = document.querySelector('[data-updated]');
    if (updated) {
      const d = new Date();
      updated.textContent = d.toLocaleDateString(undefined, { year: 'numeric', month: 'short', day: 'numeric' });
    }

    const links = Array.from(document.querySelectorAll('.toc-link'));
    const sections = links.map(a => document.querySelector(a.getAttribute('href')));

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        const i = sections.indexOf(entry.target);
        if (i >= 0) {
          const link = links[i];
          if (entry.isIntersecting) {
            links.forEach(l => l.classList.remove('text-blue-700', 'font-semibold'));
            link.classList.add('text-blue-700', 'font-semibold');
          }
        }
      });
    }, { rootMargin: '-60% 0px -35% 0px', threshold: 0 });

    sections.forEach(sec => sec && observer.observe(sec));
  </script>
</body>
</html>
