import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoConfig, AutoModel, PreTrainedModel
from pathlib import Path
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# nothing is random here so no seed is set

# code used from https://huggingface.co/desklib/ai-text-detector-v1.01 and modified for this project
class DesklibAIDetectionModel(PreTrainedModel):
    config_class = AutoConfig

    def __init__(self, config):
        # initialize the pretrained model stuff
        super().__init__(config)
        # load the base transformer model
        self.model = AutoModel.from_config(config)
        # this is the classifier head we add on top
        self.classifier = nn.Linear(config.hidden_size, 1)
        # init weights using parent class method
        self.init_weights()

    def forward(self, input_ids, attention_mask=None, labels=None):
        # run input through the transformer
        outputs = self.model(input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs[0]
        # do mean pooling to get single vector
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)
        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
        pooled_output = sum_embeddings / sum_mask

        # pass through classifier to get logits
        logits = self.classifier(pooled_output)
        loss = None
        if labels is not None:
            loss_fct = nn.BCEWithLogitsLoss()
            loss = loss_fct(logits.view(-1), labels.float())

        output = {"logits": logits}
        if loss is not None:
            output["loss"] = loss
        return output

# runs prediction on a single piece of text
def predict_single_text(text, model, tokenizer, device, max_len=768, threshold=0.5):
    encoded = tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=max_len,
        return_tensors='pt'
    )
    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs["logits"]
        probability = torch.sigmoid(logits).item()

    ai_detected = True if probability >= threshold else False
    return probability, ai_detected

# this function detects if text is ai generated or human written
# takes text string and threshold, returns probability and boolean
def ai_plagiarism_detection(text, threshold=0.5, show_results=False):
    # path to the model we're using
    model_directory = "desklib/ai-text-detector-v1.01"
    # use gpu if available otherwise cpu
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_directory)
    # load model to cpu first then move to device to avoid tensor issues
    model = DesklibAIDetectionModel.from_pretrained(
        model_directory,
        torch_dtype=torch.float32
    )
    model = model.to(device)
    # get the prediction
    probability, ai_detected = predict_single_text(text, model, tokenizer, device, threshold=threshold)
    # print stuff if user wants
    if show_results:
        print(f"ai prob: {probability:.4f}")
        print(f"result: {'ai generated' if ai_detected else 'human written'}")
    return probability, ai_detected


# helper function to create a text file with some content
def make_textfile(file_path="text_folder/example.txt", content = "This is an example text file.\nAnd this is the second line.\n"):
    # open file in write mode and write the content
    with open(file_path, "w") as f:
        f.write(content)
    return

# reads all text files from a folder and returns dict with filename -> content
def get_text_from_textfile(text_dir="text_folder"):
    text_dict = {}
    text_file_list = list(Path(text_dir).glob("*.txt"))
    for elem in text_file_list:
        content = elem.read_text(encoding="utf-8")  # read file content
        text_dict[elem.name] = content  # use filename as key
    return text_dict

# shows how to use the model with text files in a folder
# this is what we use in the pipeline
def classifying_plagiarism_using_textfiles(best_threshold=0.78):
    # make sure folder exists
    Path("text_folder").mkdir(exist_ok=True)
    
    # create example text files for testing
    make_textfile("text_folder/ai_text.txt", "AI detection refers to the process of identifying whether a given piece of content, such as text, images, or audio, has been generated by artificial intelligence. This is achieved using various machine learning techniques, including perplexity analysis, entropy measurements, linguistic pattern recognition, and neural network classifiers trained on human and AI-generated data. Advanced AI detection tools assess writing style, coherence, and statistical properties to determine the likelihood of AI involvement. These tools are widely used in academia, journalism, and content moderation to ensure originality, prevent misinformation, and maintain ethical standards. As AI-generated content becomes increasingly sophisticated, AI detection methods continue to evolve, integrating deep learning models and ensemble techniques for improved accuracy.")  # create an example text file
    make_textfile("text_folder/human_text.txt", "It is estimated that a major part of the content in the internet will be generated by AI / LLMs by 2025. This leads to a lot of misinformation and credibility related issues. That is why if is important to have accurate tools to identify if a content is AI generated or human written")  # create another example text file
    textfile_dict = get_text_from_textfile(text_dir="text_folder")  # get dict with text file and content, text_dir is folder containing text files that need to be classified

    # loop through each file and get predictions
    for textfile, text in textfile_dict.items(): # for key, value in ft_dict.items():
        print(f"checking {textfile}")
        # get predictions with the optimal threshold value
        probability, ai_detected = ai_plagiarism_detection(text=text, threshold=best_threshold, show_results=False)
        # print results
        print(f"  ai prob: {probability:.4f}")
        print(f"  verdict: {'ai' if ai_detected else 'human'}\n")



# gets text from json file, each line is a json object with text field
# sample_size controls how many we return
def get_texts_from_jsonfile(json_file_path, sample_size=100, ignore_warning=False):
    text_list = []
    try:
        with open(json_file_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                obj = json.loads(line)
                text_list.append(obj["text"])
                if i == sample_size-1:
                    break
    except:
        raise ValueError(f"{json_file_path} does not exist or is not found.")
    # warn if we got less texts than expected
    if ignore_warning != True:
        if len(text_list) != sample_size:
            raise ValueError(f"Warning: only {len(text_list)} texts found, less than sample size {sample_size}")

    return text_list

# runs the experiment using json files and saves results to csv
def run_experiment_using_jsonfile(threshold=0.5):
    # total sample size, split between two datasets
    sample_size = 240 
    sample_size //=2
    

    # make sure folders exist
    Path("json_folder").mkdir(exist_ok=True)
    Path("ai_plagiarism_experiment").mkdir(exist_ok=True)

    # get true negative texts (human spoken) from ml commons dataset
    text_list = get_texts_from_jsonfile("json_folder/ML_commons.json", sample_size)
    
    # get predictions for each text
    predictions=[]
    for i, text in enumerate(text_list):
        # run the detection
        probability, ai_detected = ai_plagiarism_detection(text=text, threshold=threshold, show_results=False)
        # save results to list
        predictions.append({"ML_commons_text_index": i,
                            "GPT_text_index": np.nan,
                            "text_length": len(text),
                            "topic": "unknown",
                            "probability": probability,
                            "ai_detected": ai_detected,
                            "really_ai": False
                            })
    # convert to dataframe
    df = pd.DataFrame(predictions)
    print("halfway done with predictions")
    
    # now get true positive texts (ai written) from gpt dataset
    text_list = get_texts_from_jsonfile("json_folder/gpt_generated.json", sample_size)

    predictions=[]
    for i, text in enumerate(text_list):
        # run detection
        probability, ai_detected = ai_plagiarism_detection(text=text, threshold=threshold, show_results=False)

        # figure out topic based on index
        if i < 40:
            topic = "astronomy"
        elif i < 80:
            topic = "quantum computing"
        else:
            topic = "daily life, personal growth, and everyday experiences"

        predictions.append({"ML_commons_text_index": np.nan,
                            "GPT_text_index": i,
                            "text_length": len(text),
                            "topic": topic,
                            "probability": probability,
                            "ai_detected": ai_detected,
                            "really_ai": True
                            })
    # add new rows to dataframe
    new_rows = pd.DataFrame(predictions)
    df = pd.concat([df, new_rows], ignore_index=True)
    print("all predictions done")
    # save to csv
    df.to_csv("ai_plagiarism_experiment/ai_plagiarism_detection_results.csv", index=False)

    # update metrics
    get_metrics(threshold=threshold)


# calculates metrics like accuracy precision recall from the results
def get_metrics(df=None, threshold=0.5, save_to_csv=True):
    if df is None:
        # read from csv if not provided
        df = pd.read_csv("ai_plagiarism_experiment/ai_plagiarism_detection_results.csv")

    # calculate all the metrics
    fp = ((df["probability"]>=threshold) & (df["really_ai"]==False)).sum()  # false positives
    tn = ((df["probability"]<threshold) & (df["really_ai"]==False)).sum()   # true negatives
    tp = ((df["probability"]>=threshold) & (df["really_ai"]==True)).sum()   # true positives
    fn = ((df["probability"]<threshold) & (df["really_ai"]==True)).sum()    # false negatives

    recall = tp/(tp+fn) if (tp+fn) != 0 else 0
    precision = tp/(tp+fp) if (tp+fp) != 0 else 0
    accuracy = (tp+tn)/(tp+fp+tn+fn) if (tp+fp+tn+fn) != 0 else 0

    # get some info about text lengths
    ML_commons_length_mean = df.loc[df["ML_commons_text_index"].notna(), "text_length"].mean()
    ML_commons_length_std = df.loc[df["ML_commons_text_index"].notna(), "text_length"].std()
    gpt_length_mean = df.loc[df["GPT_text_index"].notna(), "text_length"].mean()
    gpt_length_std = df.loc[df["GPT_text_index"].notna(), "text_length"].std()
        

    # save metrics in dataframe
    results = pd.DataFrame({
        "Metric": ["TP", "TN", "FP", "FN", "Recall", "Precision", "Accuracy", "Total samples", "ML_commons_length_mean", "ML_commons_length_std", "gpt_length_mean", "gpt_length_std"],
        "Value": [tp, tn, fp, fn, recall, precision, accuracy, len(df), ML_commons_length_mean, ML_commons_length_std, gpt_length_mean, gpt_length_std]
    })
    if save_to_csv:
        # save in csv
        results.to_csv(f"ai_plagiarism_experiment/res_metrics(t={threshold}).csv", index=False) 
    return results
    
# finds the best threshold value by trying different values
def tune_threshold(metric = "Accuracy"):
    df = pd.read_csv("ai_plagiarism_experiment/ai_plagiarism_detection_results.csv")
    # set boundaries for search
    min = 0.0
    max = 1.0
    step = 0.01
    # init vars
    best_accuracy=0
    m_l=[]
    t_l=[]
    for threshold in np.arange(min, max+step, step):
        threshold = round(threshold, 2)
        results = get_metrics(df,threshold,False)
        opti_metric = results.loc[results["Metric"] == metric, "Value"].iloc[0]
        # save for plotting
        m_l.append(opti_metric)
        t_l.append(threshold)
        # update best threshold if we found a better one
        if opti_metric>best_accuracy:
            best_accuracy = opti_metric
            best_threshold = threshold

    # plot tuning results
    Path("ai_plagiarism_tuning_plots").mkdir(exist_ok=True)
    plt.plot(t_l, m_l)
    plt.xlabel("threshold")
    plt.ylabel(metric)
    plt.title(f"threshold vs {metric}")
    plt.savefig(f"ai_plagiarism_tuning_plots/threshold_vs_{metric}.png")
    plt.close()
    return best_threshold


        
        

if __name__ == "__main__":
    print("starting experiment\n")    
    # run experiment using json files
    run_experiment_using_jsonfile(threshold=0.5) # firstly using the default threshold

    # search for the theshold that maximises accuracy
    metric = "Accuracy"
    best_threshold_accuracy = tune_threshold(metric=metric)
    print(f"best threshold for {metric}: {best_threshold_accuracy}")
    # search for the theshold that maximises precision
    metric = "Precision"
    best_threshold_precision = tune_threshold(metric=metric)
    print(f"best threshold for {metric}: {best_threshold_precision}")

    # run experiment using json files
    run_experiment_using_jsonfile(threshold=best_threshold_accuracy) # secondly using the optimal threshold, the end result is 
    
    # example of usage that is fit for a pipeline using the best accuracy (best_threshold=0.78), when using best precision use best_threshold=0.97
    classifying_plagiarism_using_textfiles(best_threshold=best_threshold_accuracy)

    print("\ndone")
